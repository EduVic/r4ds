{
  "hash": "4eb4b7b6c7e2a02d1212dddbff73f1d2",
  "result": {
    "markdown": "---\nfreeze: true\n---\n\n\n# Binary data formats {#sec-binary-data}\n\n\n\n:::: status\n::: callout-note \nYou are reading the work-in-progress second edition of R for Data Science. This chapter should be readable but is currently undergoing final polishing. You can find the complete first edition at <https://r4ds.had.co.nz>.\n:::\n::::\n\n\n## Introduction\n\nCSV files are designed to be easily read by humans.\nThey're a good interchange format because they're very simple.\nBut they're not very efficient: we have to do quite a lot of work to read the data into a format that R can work with.\nIn this chapter, you'll learn more about binary data formats that are specifically designed to work with large datasets.\n\nTODO: say something about databases.\n\nIn particular, we'll focus on the [parquet format](https://parquet.apache.org/), an open standards-based format widely used by big data systems.\nWe'll pair parquet with [Apache Arrow](https://arrow.apache.org), a multi-language toolbox designed for efficient analysis and transport of large data sets.\nThe [arrow package](https://arrow.apache.org/docs/r/) provides a standard way to use Apache Arrow in R, and allows you to analyze larger-than-memory datasets using familiar dplyr syntax.\nAs an additional benefit, arrow is extremely fast: you'll see some examples later in the chapter.\n\n### Prerequisites\n\nThis chapter focuses on large data sets, and binary file formats that are well suited to working with them.\nWe'll continue to use the tidyverse, particularly dplyr, but now we'll pair it with the arrow package which is designed specifically for working with large data.\nWe'll also\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(arrow)\n\nlibrary(dbplyr)\nlibrary(duckdb)\n```\n:::\n\n\n## Getting the data\n\nWe'll also need a dataset worthy of these tools.\nWe're going to use a data set of item checkouts from Seattle public libraries, available online at [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6).\nThe data is distributed as a single 9GB csv file, so it may take some time to download: this is one of the challenges of working with bigger datasets!\n\nI've made the data available on Amazon's S3 service to ensure that we're all working with the same data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndir.create(\"data\", showWarnings = FALSE)\nurl <- \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\"\n\n# Default timeout is 60s; bump it up to an hour\noptions(timeout = 60 * 60)\ndownload.file(url, \"data/seattle-library-checkouts.csv\")\n```\n:::\n\n\nTODO: describe the data.\nNumber of rows.\nWhat the columns are.\n\n## Opening a dataset\n\nLet's start by taking a look at the Seattle library checkout data.\nAt 9GB in size, the csv file is large enough that we probably don't want to load the whole thing into memory: a good rule of thumb is that you usually want at least twice as much memory as the size of the data.\nSo instead of using `read_csv()` we'll use the `open_dataset()` function from the arrow package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  format = \"csv\",\n  schema = schema(\n    field(\"UsageClass\", string()),\n    field(\"CheckoutType\", string()),\n    field(\"MaterialType\", string()),\n    field(\"CheckoutYear\", int64()),\n    field(\"CheckoutMonth\", int64()),\n    field(\"Checkouts\", int64()),\n    field(\"Title\", string()),\n    field(\"ISBN\", string()),\n    field(\"Creator\", string()),\n    field(\"Subjects\", string()),\n    field(\"Publisher\", string()),\n    field(\"PublicationYear\", string())\n  ),\n  skip_rows = 1\n)\n```\n:::\n\n\nWhen this code is run, `open_dataset()` scans the first part of the file: it reads enough rows to work out the structure of the data set structure and creates the `seattle_csv` object that stores that metadata.\nThe data remain on-disk, and are not loaded into memory, unless you force a computation with `collect()` or `glimpse()`:\n\n\n::: {.cell hash='binary-data-formats_cache/html/glimpse-data_92fd4b9a1bfa09086be5aef89856fddc'}\n\n```{.r .cell-code}\nseattle_csv |> glimpse()\n#> FileSystemDataset with 1 csv file\n#> 41,389,465 rows x 12 columns\n#> $ UsageClass      <string> \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Ph…\n#> $ CheckoutType    <string> \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Hor…\n#> $ MaterialType    <string> \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOO…\n#> $ CheckoutYear     <int64> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 20…\n#> $ CheckoutMonth    <int64> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n#> $ Checkouts        <int64> 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2,…\n#> $ Title           <string> \"Super rich : a guide to having it all / Russell S…\n#> $ ISBN            <string> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Creator         <string> \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim …\n#> $ Subjects        <string> \"Self realization, Conduct of life, Attitude Psych…\n#> $ Publisher       <string> \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Di…\n#> $ PublicationYear <string> \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c20…\n```\n:::\n\n\nThe first line in the output tell you that `seattle_csv` is stored locally on-disk as a single csv file.\nThe second line indicates that the data are stored as a tabular data set with 41 million rows and 12 columns.\n\nWe can start to immediately use this dataset with dplyr verbs, using `collect()` to force arrow to perform the computation and return some data:\n\n\n::: {.cell hash='binary-data-formats_cache/html/unnamed-chunk-3_2bf8d92538b67d301aa4f0a627d5bcd4'}\n\n```{.r .cell-code}\nseattle_csv |> \n  count(CheckoutYear) |> \n  collect()\n#> # A tibble: 18 × 2\n#>   CheckoutYear       n\n#>          <int>   <int>\n#> 1         2016 2787319\n#> 2         2022 2113271\n#> 3         2017 2820433\n#> 4         2018 2665098\n#> 5         2019 2589001\n#> 6         2020 1721376\n#> # … with 12 more rows\n```\n:::\n\n\nThis already shows a substantial benefit of arrow: this code will work regardless of how large the underlying dataset is.\nBut this is currently rather slow: on my computer, the code above takes about 10s to run.\nThat's not terrible given how much data we have, but we can make it much faster by switching to a better format.\n\n## A better format\n\nTo make this data easier to work with, we're going to do two things: we're going to use the parquet file format and we're going to split it up into multiple files.\nOnce we've introduced you to parquet and partitioning, we'll apply what we've learned to the Seattle library data.\n\n### Parquet\n\nLike CSV, parquet stores a rectangular data set, but it's designed specifically for the needs of big data.\nThis means that:\n\n-   Parquet files are usually smaller.\n    Parquet relies on [efficient encodings](https://parquet.apache.org/docs/file-format/data-pages/encodings/) to keep file size down, and supports file compression.\n    This helps make parquet files fast because there's less data to move from disk to memory.\n\n-   Parquet files have a rich type system.\n    As we talked about in @sec-col-types, a CSV file does not provide any information about column types.\n    For example, a CSV reader has to guess whether `\"08-10-2022\"` should be parsed as a string or a date.\n    In contrast, parquet files store data in a way that records the type along with the data.\n\n-   Parquet files are \"column-oriented\".\n    This means that they're organised column-by-column, much like R's data frame.\n    This typically leads to better performance for data analysis tasks compared to CSV files, which are organised row-by-row.\n\n-   Each parquet file is made up of chunks of rows.\n    This makes it possible to work on different parts of the file in parallel, and, if you're lucky, to skip some chunks all together.\n\n### Partitioning\n\nAs datasets get larger and larger, storing all the data in a single file gets increasingly painful, and it's often useful to split large datasets across many files.\nWhen this structuring is done intelligently, this strategy can lead to significant improvements in performance because many analyses will only require a subset of the files.\n\nThere are no hard and fast rules about how to partition a data set: the results will depend on your data, access patterns, and the systems that read the data.\nAs a rough guide, the arrow package documentation suggests the following heuristics:\n\n-   Avoid files smaller than 20MB and larger than 2GB.\n-   Avoid partitioning layouts that produce more than 10,000 distinct files.\n\nYou should also try to partitioning by variables that you are likely to filter by; as you'll see shortly, that allows arrow to skip a lot of work by reading only the relevant files.\n\n### Rewriting the Seattle library data\n\nLet's apply these ideas to the Seattle library data to see how they play out in practice.\nWe need to first decide how we're going to partition the data.\nWe're going to use `CheckoutYear`, since it's likely some analyses will only want to look at recent data, and it yields 18 chunks of a reasonable size.\n\nWith that in hand we can partition the data and save it as a parquet file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npq_path <- \"data/seattle-library-checkouts\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  write_dataset(path = path, format = \"parquet\")\n```\n:::\n\n\nThere are two steps: we first define our partitioning using `dplyr::group_by()` and then save the partitions to a directory using `arrow::write_dataset()`.\n`write_dataset()` has two important arguments: a directory where we'll create the files and the format we'll use.\nThis takes about a minute on my computer: this is an initial investment that's going to pay off by making all future operations much much faster.\n\nLet's take a look at what we just made:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  files = list.files(pq_path, recursive = TRUE),\n  size_MB = file.size(file.path(pq_path, files)) / 1024^2\n)\n#> # A tibble: 18 × 2\n#>   files                            size_MB\n#>   <chr>                              <dbl>\n#> 1 CheckoutYear=2005/part-0.parquet    109.\n#> 2 CheckoutYear=2006/part-0.parquet    164.\n#> 3 CheckoutYear=2007/part-0.parquet    178.\n#> 4 CheckoutYear=2008/part-0.parquet    195.\n#> 5 CheckoutYear=2009/part-0.parquet    214.\n#> 6 CheckoutYear=2010/part-0.parquet    222.\n#> # … with 12 more rows\n```\n:::\n\n\nOur single 9GB csv file has been rewritten into 18 parquet files.\nThe file names use a \"self-describing\" convention used by the [Apache Hive](https://hive.apache.org) project.\nHive-style partitions name folders with a \"key=value\" convention, so as you might guess the `CheckoutYear=2005` directory contains all the data where `CheckoutYear` is 2005.\nIt's also worth noting the total size which is now around 4 GB, a little over half the size of the original CSV file.\nThis is to be expected as parquet is a much more efficient format.\n\n## Using dplyr with arrow\n\nNow we've create these parquet files, we'll want to read them in again.\nWe use `open_dataset()` again, but this time we give it a directory:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq <- open_dataset(pq_path)\n```\n:::\n\n\nNow we can write our dplyr pipeline.\nFor our example we'll count the total number of books checked out in each month for the last five years:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery <- seattle_pq |> \n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear, CheckoutMonth) |>\n  summarise(TotalCheckouts = sum(Checkouts)) |>\n  arrange(CheckoutYear, CheckoutMonth)\n```\n:::\n\n\nWriting dplyr code for arrow data is conceptually similar to dbplyr, @sec-import-databases: you write dplyr code, which is automatically transformed into a query that the Apache Arrow C++ library understands, which is executed when you call `collect()`.\nIf we print out the `query` object we can see a little information about what we expect Arrow to return when the execution takes place:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery\n#> FileSystemDataset (query)\n#> CheckoutYear: int32\n#> CheckoutMonth: int64\n#> TotalCheckouts: int64\n#> \n#> * Grouped by CheckoutYear\n#> * Sorted by CheckoutYear [asc], CheckoutMonth [asc]\n#> See $.data for the source Arrow object\n```\n:::\n\n\nThis looks right, so we can now go ahead and call `collect()` to evaluate the query:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery |> collect()\n#> # A tibble: 58 × 3\n#> # Groups:   CheckoutYear [5]\n#>   CheckoutYear CheckoutMonth TotalCheckouts\n#>          <int>         <int>          <int>\n#> 1         2018             1         355101\n#> 2         2018             2         309813\n#> 3         2018             3         344487\n#> 4         2018             4         330988\n#> 5         2018             5         318049\n#> 6         2018             6         341825\n#> # … with 52 more rows\n```\n:::\n\n\nLike dbplyr, arrow only understands some R expressions, so you may not be able to write exactly the same code you usually would.\nHowever, the list of operations and functions supported is fairly extensive and growing.\nYou can find a complete list in `?acero`.\n\n### Performance {#sec-parquet-fast}\n\nLet's take a quick look at the performance impact of switching from CSV to parquet.\nFirst, let's time how long it takes to calculate the number of books checked out in each month of 2021, when the data set is stored as a single large csv:\n\n\n::: {.cell hash='binary-data-formats_cache/html/dataset-performance-csv_97db9621db9ade3c138cb45aa32e9cf2'}\n\n```{.r .cell-code}\nseattle_csv |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarise(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>  12.102   1.186  11.517\n```\n:::\n\n\nNow let's use our new version of the data set in which the Seattle library checkout data has been partitioned into 18 smaller parquet files:\n\n\n::: {.cell hash='binary-data-formats_cache/html/dataset-performance-multiple-parquet_6bb34ab380513ccfc9ef3750b8818f0c'}\n\n```{.r .cell-code}\nseattle_pq |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarise(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>   0.273   0.076   0.065\n```\n:::\n\n\nThe \\~100x speedup in performance is attributable to two factors: the multi-file partitioning, and the format of individual files:\n\n-   Partitioning improves performance because this query uses `CheckoutYear == 2021` to filter the data, and arrow is smart enough to recognize that it only needs to read 1 of the 18 parquet files.\n-   The parquet format improves performance by storing data in a binary format that can be read more directly into memory and because the data are stored column-wise within the file. The parquet file format contains enough metadata that arrow only needs to read the four columns actually used in this query (`CheckoutYear`, `MaterialType`, `CheckoutMonth`, and `Checkouts`).\n\nThis massive difference is performance is why that whenever you're working with large CSVs, you should always convert to parquet first!\n\n### Using dbplyr with arrow\n\nThere's one last advantage of parquet and arrow that we wanted point you --- it's very easy to turn an arrow dataset into a duckdb datasource by calling `arrow::to_duckdb()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq |> \n  to_duckdb() |>\n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear) |>\n  summarise(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutYear)) |>\n  collect()\n#> Warning: Missing values are always removed in SQL aggregation functions.\n#> Use `na.rm = TRUE` to silence this warning\n#> This warning is displayed once every 8 hours.\n#> # A tibble: 5 × 2\n#>   CheckoutYear TotalCheckouts\n#>          <int>          <dbl>\n#> 1         2022        2431502\n#> 2         2021        2266438\n#> 3         2020        1241999\n#> 4         2019        3931688\n#> 5         2018        3987569\n```\n:::\n\n\nThe nice thing about `to_duckdb()` is that the transfer doesn't involve any memory copying, and speaks to the goals of the arrow ecosystem: enable seamless transitions from one computing framework to another.\n\n## Summary\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}