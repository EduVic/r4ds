{
  "hash": "1ff8121d7f9270a6010d0a4d5bdea94d",
  "result": {
    "markdown": "---\nfreeze: true\n---\n\n\n# Binary data formats {#sec-binary-data}\n\n\n\n:::: status\n::: callout-important \nYou are reading the work-in-progress second edition of R for Data Science. This chapter is currently a dumping ground for ideas, and we don't recommend reading it. You can find the complete first edition at <https://r4ds.had.co.nz>.\n:::\n::::\n\n\n## Introduction\n\n### Prerequisites\n\nThis chapter focuses on large data sets, and binary file formats that are well suited to working with them.\nWe'll use a data set of item checkouts from Seattle public libraries, available online at [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6).\nThe data is distributed as a single 9GB csv file, so it may take some time to download.\nFor the purposes of this chapter we'll assume that you have a local copy of the file at `data/seattle-library-checkout.csv`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(arrow)\nlibrary(duckdb)\nlibrary(dbplyr)\n```\n:::\n\n\n## The arrow package\n\nIn this chapter we'll rely on Apache Arrow ([arrow.apache.org](https://arrow.apache.org/)), a multi-language toolbox designed for efficient analysis and transport of large data sets.\nThe [arrow package](https://arrow.apache.org/docs/r/) provides a standard way to use Apache Arrow in R, and allows you to analyze larger-than-memory datasets using familiar dplyr syntax.\nAs an additional benefit, arrow is extremely fast: you'll see some examples later in the chapter.\n\n### Opening datasets\n\nLet's start by taking a look at the Seattle library checkout data.\nAt 9GB in size, the csv file is large enough that we probably don't want to load the whole thing into memory: a good rule of thumb is that you usually want at least twice as much memory as the size of the data.\nInstead of using `read_csv()` we'll use the `open_dataset()` function from the arrow package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_lib <- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n)\n```\n:::\n\n\nWhen this code is run, `open_dataset()` scans the first part of the file: it reads enough rows to work out the structure of the data set structure and creates the `seattle_lib` object that stores metadata.\nThe data remain on-disk, and are not loaded into memory.\nNevertheless, we are able to call dplyr functions to interact with the data:\n\n\n::: {.cell hash='binary-data-formats_cache/html/glimpse-data_75d5cfe6cbb12e2777b896993db1e015'}\n\n```{.r .cell-code}\nglimpse(seattle_lib)\n#> FileSystemDataset with 1 csv file\n#> 41,389,465 rows x 12 columns\n#> $ UsageClass      <string> \"Physical\", \"Physical\", \"Physical\", \"Physical\", \"Phys…\n#> $ CheckoutType    <string> \"Horizon\", \"Horizon\", \"Horizon\", \"Horizon\", \"Horizon\"…\n#> $ MaterialType    <string> \"BOOK\", \"SOUNDDISC\", \"SOUNDDISC\", \"BOOK\", \"VIDEODISC\"…\n#> $ CheckoutYear     <int64> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022,…\n#> $ CheckoutMonth    <int64> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n#> $ Checkouts        <int64> 2, 1, 1, 4, 4, 1, 1, 2, 1, 3, 2, 7, 3, 2, 1, 3, 3, 1,…\n#> $ Title           <string> \"Ten count. Volume 5 / story and art by Rihito Takara…\n#> $ ISBN            <string> \"1421593734, 9781421593739\", \"\", \"0063016141, 9780063…\n#> $ Creator         <string> \"Takarai, Rihito\", \"Cassidy, Eva\", \"Hampton, Dan\", \"P…\n#> $ Subjects        <string> \"Comic books strips etc Japan Translations into Engli…\n#> $ Publisher       <string> \"SuBLime Manga,\", \"Blix Street Records,\", \"Harper Aud…\n#> $ PublicationYear <string> \"[2017]\", \"[1998]\", \"[2020]\", \"2019.\", \"[2022]\", \"[20…\n```\n:::\n\n\nThe first line in the output tell you that `seattle_lib` is stored locally on-disk as a single csv file.\nThe second line indicates that the data are stored as a tabular data set with 41 million rows and 12 columns.\n\n### Partitioning datasets\n\nWhen data sets become large it is often useful to split them across many files rather than storing everything in a single large file.\nWhen this structuring is done intelligently, the multi-file strategy can lead to significant improvements in performance because some analyses will only require a subset of the files to be read.\n\nWhen deciding to pursue a multi-file strategy, the first task is to decide how to partition your data set.\nIf you break the data set into too few files you won't gain the performance improvements you're hoping for, but if you break it into too many you won't gain much either.\nIf you split the data in a meaningful way, you'll get some performance advantages, but if you split the data randomly you probably won't see a lot of gains.\nThere are no hard and fast rules about how to partition a data set: the results will depend on your data, access patterns, and the systems that read the data.\nAs a rough guide, the arrow package documentation suggests the following heuristics:\n\n-   Avoid files smaller than 20MB and larger than 2GB.\n-   Avoid partitioning layouts that produce more than 10,000 distinct files.\n\nAs a general rule, a useful strategy for partitioning a large dataset is to create meaningful subsets based on one or more variables that you're likely to use to filter the data.\n\n### Writing datasets\n\nThe guidelines in the last section suggest that the Seattle library data is a little too large to store as a single file.\nHow should we split it up?\nIt's reasonable to guess that our analyses might filter the data by `CheckoutYear`, so we'll use that as our partitioning variable.\n\nPartitioning a data set can be done using the `write_dataset()` function from the arrow package, which respects the grouping created by the dplyr `group_by()` function.\nTo create a multi-file data set that contains a separate file for each year of library checkout data, we can do this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_lib |>\n  group_by(CheckoutYear) |>\n  write_dataset(\n    path = \"data/seattle-library-checkouts\",\n    format = \"parquet\"\n  )\n```\n:::\n\n\nThe `path` argument is used to specify the folder into which all the data files will be written, and the `format` argument is used to specify the file format for the data files.\nWe'll talk more about the parquet format in @sec-parquet.\nFor the moment, let's take a look at the files:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_lib_files <- list.files(\n  path = \"data/seattle-library-checkouts\",\n  recursive = TRUE\n) \nseattle_lib_files\n#>  [1] \"CheckoutYear=2005/part-0.parquet\" \"CheckoutYear=2006/part-0.parquet\"\n#>  [3] \"CheckoutYear=2007/part-0.parquet\" \"CheckoutYear=2008/part-0.parquet\"\n#>  [5] \"CheckoutYear=2009/part-0.parquet\" \"CheckoutYear=2010/part-0.parquet\"\n#>  [7] \"CheckoutYear=2011/part-0.parquet\" \"CheckoutYear=2012/part-0.parquet\"\n#>  [9] \"CheckoutYear=2013/part-0.parquet\" \"CheckoutYear=2014/part-0.parquet\"\n#> [11] \"CheckoutYear=2015/part-0.parquet\" \"CheckoutYear=2016/part-0.parquet\"\n#> [13] \"CheckoutYear=2017/part-0.parquet\" \"CheckoutYear=2018/part-0.parquet\"\n#> [15] \"CheckoutYear=2019/part-0.parquet\" \"CheckoutYear=2020/part-0.parquet\"\n#> [17] \"CheckoutYear=2021/part-0.parquet\" \"CheckoutYear=2022/part-0.parquet\"\n```\n:::\n\n\nOur single 9GB csv file has been rewritten as 18 parquet files that are typically between 150MB and 300MB in size.\n\nThe file names shown in the output above follow a \"self-describing\" convention used by the Apache Hive project ([hive.apache.org](https://hive.apache.org/)).\nHive-style partitions name folders using a \"key=value\" convention, and the data files in that folder contain the subset of the data for which the key (in this case \"CheckoutYear\") has the relevant value (e.g., 2009, 2010, etc).\nFollowing this naming convention makes it easier to locate files relevant to a particular query.\n\n### Using dplyr with arrow\n\nWriting dplyr code for arrow data is conceptually similar to dbplyr, described in @sec-import-databases.\nThe dplyr code that you write in R is internally transformed to a query that the Apache Arrow C++ library understands, which is then executed when you call `collect()`.\nLet's work through an example.\n\nWe'll start by opening the partitioned parquet version of the dataset created in the last section.\nAs we'll discuss in @sec-parquet-fast, our analysis runs much faster using this version of the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_lib2 <- open_dataset(\"data/seattle-library-checkouts/\")\n```\n:::\n\n\nNext we'll write our dplyr pipeline.\nFor our example we'll count the total number of books checked out in each year for the last five years:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery <- seattle_lib2 |> \n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear) |>\n  summarise(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutYear))\n```\n:::\n\n\nJust like dbplyr, this pipeline defines instructions that arrow will execute when requested.\nIf we print out the `query` object we can see a little information about what we expect Arrow to return when the execution takes place:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery\n#> FileSystemDataset (query)\n#> CheckoutYear: int32\n#> TotalCheckouts: int64\n#> \n#> * Sorted by CheckoutYear [desc]\n#> See $.data for the source Arrow object\n```\n:::\n\n\nThis looks right, so we can now go ahead and call `collect()` to evaluate the query:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect(query)\n#> # A tibble: 5 × 2\n#>   CheckoutYear TotalCheckouts\n#>          <int>          <int>\n#> 1         2022        2431502\n#> 2         2021        2266438\n#> 3         2020        1241999\n#> 4         2019        3931688\n#> 5         2018        3987569\n```\n:::\n\n\nLike dbplyr, arrow only understands some R expressions, so you may not be able to write exactly the same code you usually would.\nHowever, the list of operations and functions supported is fairly extensive and growing.\nA complete list is provided in the arrow package documentation at [arrow.apache.org/docs/r/reference/acero.html](https://arrow.apache.org/docs/r/reference/acero.html).\n\n### Using dbplyr with arrow\n\nThere's also an even more direct connection between arrow and dbplyr -- it's very easy to turn an arrow dataset into a duckdb datasource using the arrow `to_duckdb()` function (and vice versa with `to_arrow()`).\nTo illustrate this, let's run the analysis from the last section using duckdb to do the computing work:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_lib2 |> \n  to_duckdb() |>\n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear) |>\n  summarise(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutYear)) |>\n  collect()\n#> Warning: Missing values are always removed in SQL aggregation functions.\n#> Use `na.rm = TRUE` to silence this warning\n#> This warning is displayed once every 8 hours.\n#> # A tibble: 5 × 2\n#>   CheckoutYear TotalCheckouts\n#>          <int>          <dbl>\n#> 1         2022        2431502\n#> 2         2021        2266438\n#> 3         2020        1241999\n#> 4         2019        3931688\n#> 5         2018        3987569\n```\n:::\n\n\nThe nice thing about `to_duckdb()` is that the transfer doesn't involve any memory copying, and speaks to the goals of the arrow ecosystem: enable seamless transitions from one computing framework to another.\n\n## The parquet file format {#sec-parquet}\n\nWhen working with small data sets csv files can be convenient, but they may not be the best choice for larger data sets.\nThe Apache Parquet file format ([parquet.apache.org](https://parquet.apache.org/)) is a open standards-based format widely used by big data systems that is often more useful.\nLike a csv file, a parquet file stores a single rectangular data set, but differs in several important respects:\n\n-   Parquet files are usually smaller.\n    A csv file is a plain text format, and tends to be larger than file formats that use more economical coding and support compression.\n    Parquet relies on [efficient encodings](https://parquet.apache.org/docs/file-format/data-pages/encodings/) to keep file size down, and supports file compression.\n\n-   Parquet files have a rich type system.\n    As we talked about in @sec-col-types, a csv file does not provide any information about column types.\n    For example, a csv reader has to guess whether `\"08-10-2022\"` should be parsed as a string or a date.\n    In contrast, parquet files store data in a binary format (not plain text) that supports a rich type system allowing data to be read without the guesswork that csv reading requires.\n\n-   Parquet files are structured.\n    Internally, parquet files break the data into distinct chunks, each with associated metadata.\n    The inclusion of this metadata makes it possible to write economical file readers that only read those parts of the file that contain relevant data.\n\n### Parquet files are faster {#sec-parquet-fast}\n\nThe format of your data file can have a substantial impact on how long a data analysis pipeline takes to execute.\nWe'll use the Seattle library data set to illustrate.\nFirst, let's time how long it takes to calculate the number of books checked out in each month of 2021, when the data set is stored as a single large csv:\n\n\n::: {.cell hash='binary-data-formats_cache/html/dataset-performance-csv_5f73293bf68b845adbe3198fc11801d9'}\n\n```{.r .cell-code}\nsystem.time({\n  open_dataset(\n    sources = \"data/seattle-library-checkouts.csv\", \n    format = \"csv\"\n  ) |>\n    filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n    group_by(CheckoutMonth) |>\n    summarise(TotalCheckouts = sum(Checkouts)) |>\n    arrange(desc(CheckoutMonth)) |>\n    collect()\n})\n#>    user  system elapsed \n#>  14.027   2.489  14.158\n```\n:::\n\n\nNow let's use our new version of the data set in which the Seattle library checkout data has been partitioned into 18 smaller parquet files:\n\n\n::: {.cell hash='binary-data-formats_cache/html/dataset-performance-multiple-parquet_99625178da5d3354e2f84f60118a70ec'}\n\n```{.r .cell-code}\nsystem.time({\n  open_dataset(\n    sources = \"data/seattle-library-checkouts/\", \n    format = \"parquet\"\n  ) |>\n    filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n    group_by(CheckoutMonth) |>\n    summarise(TotalCheckouts = sum(Checkouts)) |>\n    arrange(desc(CheckoutMonth)) |>\n    collect()\n})\n#>    user  system elapsed \n#>   0.430   0.063   0.157\n```\n:::\n\n\nThe 100x speedup in performance is attributable to two factors: the multi-file partitioning, and the file format itself.\nThe partitioning improves performance because our query uses `CheckoutYear == 2021` to filter the data, and arrow is smart enough to recognize that it only needs to read 1 of the 18 parquet files.\n\nThe file format improves performance because parquet files are smaller than the corresponding csv file (so there's less to read off the disk), and because the data are stored column-wise within the file.\nFor the query we wrote above, there are only 4 of the 12 columns that need to be read (`CheckoutYear`, `MaterialType`, `CheckoutMonth`, and `Checkouts`).\nThe parquet file format contains metadata that allows readers to skip straight to those columns without reading the whole file.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}