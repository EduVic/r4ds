{
  "hash": "6aa3c3284f64b5ca3d3dc64edfd1c9b5",
  "result": {
    "markdown": "---\nfreeze: true\n---\n\n\n# Binary data formats {#sec-binary-data}\n\n\n::: callout-important\nYou are reading the work-in-progress second edition of R for Data Science. This chapter is currently a dumping ground for ideas, and we don't recommend reading it. You can find the complete first edition at <https://r4ds.had.co.nz>.\n:::\n\n\n## Introduction\n\n### Prerequisites\n\nThis chapter focuses on large data sets, and binary file formats that are well suited to working with them.\nWe'll use a data set of item checkouts from Seattle public libraries, available online at [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6).\nThe data is distributed as a single 9GB csv file, so it may take some time to download.\nFor the purposes of this chapter we'll assume that you have a local copy of the file at `data/seattle-library-checkout.csv`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(arrow)\nlibrary(duckdb)\nlibrary(dbplyr)\n```\n:::\n\n\n## The arrow package\n\nIn this chapter we'll rely on Apache Arrow ([arrow.apache.org](https://arrow.apache.org/)), a multi-language toolbox designed for efficient analysis and transport of large data sets.\nThe [arrow package](https://arrow.apache.org/docs/r/) provides a standard way to use Apache Arrow in R, and allows you to analyze larger-than-memory datasets using familiar dplyr syntax.\nAs an additional benefit, arrow is extremely fast: you'll see some examples later in the chapter.\n\n### Opening a dataset\n\nLet's start by taking a look at the Seattle library checkout data.\nAt 9GB in size, the csv file is large enough that we probably don't want to load the whole thing into memory: a good rule of thumb is that you usually want at least twice as much memory as the size of the data.\nInstead of using `read_csv()` we'll use the `open_dataset()` function from the arrow package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_lib <- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n)\n```\n:::\n\n\nWhen this code is run, `open_dataset()` scans the first part of the file: it reads enough rows to work out the structure of the data set structure and creates the `seattle_lib` object that stores metadata.\nThe data remain on-disk, and are not loaded into memory.\nNevertheless, we are able to call dplyr functions to interact with the data:\n\n\n::: {.cell hash='binary-data-formats_cache/html/glimpse-data_75d5cfe6cbb12e2777b896993db1e015'}\n\n```{.r .cell-code}\nglimpse(seattle_lib)\n#> FileSystemDataset with 1 csv file\n#> 41,389,465 rows x 12 columns\n#> $ UsageClass      <string> \"Physical\", \"Physical\", \"Physical\", \"Physical\", \"Phys…\n#> $ CheckoutType    <string> \"Horizon\", \"Horizon\", \"Horizon\", \"Horizon\", \"Horizon\"…\n#> $ MaterialType    <string> \"BOOK\", \"SOUNDDISC\", \"SOUNDDISC\", \"BOOK\", \"VIDEODISC\"…\n#> $ CheckoutYear     <int64> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022,…\n#> $ CheckoutMonth    <int64> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n#> $ Checkouts        <int64> 2, 1, 1, 4, 4, 1, 1, 2, 1, 3, 2, 7, 3, 2, 1, 3, 3, 1,…\n#> $ Title           <string> \"Ten count. Volume 5 / story and art by Rihito Takara…\n#> $ ISBN            <string> \"1421593734, 9781421593739\", \"\", \"0063016141, 9780063…\n#> $ Creator         <string> \"Takarai, Rihito\", \"Cassidy, Eva\", \"Hampton, Dan\", \"P…\n#> $ Subjects        <string> \"Comic books strips etc Japan Translations into Engli…\n#> $ Publisher       <string> \"SuBLime Manga,\", \"Blix Street Records,\", \"Harper Aud…\n#> $ PublicationYear <string> \"[2017]\", \"[1998]\", \"[2020]\", \"2019.\", \"[2022]\", \"[20…\n```\n:::\n\n\nThe first line in the output tell you that `seattle_lib` is stored locally on-disk as a single csv file.\nThe second line indicates that the data are stored as a tabular data set with 41 million rows and 12 columns.\n\n### Using dplyr with arrow\n\nWriting dplyr code for arrow data is conceptually similar to dbplyr, described in @sec-import-databases.\nThe dplyr code that you write in R is internally transformed to a query that the Apache Arrow C++ library understands, which is then executed when you call `collect()`.\nHere's an example that counts the total number of books checked out in each year:\n\n\n::: {.cell hash='binary-data-formats_cache/html/books-by-year_8f33b08427cd093a5d266ee5d920e380'}\n\n```{.r .cell-code}\nseattle_lib |> \n  filter(MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear) |>\n  summarise(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutYear)) |>\n  collect()\n#> # A tibble: 18 × 2\n#>   CheckoutYear TotalCheckouts\n#>          <int>          <int>\n#> 1         2022        2431502\n#> 2         2021        2266438\n#> 3         2020        1241999\n#> 4         2019        3931688\n#> 5         2018        3987569\n#> 6         2017        3971593\n#> # … with 12 more rows\n```\n:::\n\n\nLike dbplyr, arrow only understands some R expressions, so you may not be able to write exactly the same code you usually would.\nHowever, the list of operations and functions supported is fairly extensive and growing.\nA complete list is provided in the arrow package documentation at [arrow.apache.org/docs/r/reference/acero.html](https://arrow.apache.org/docs/r/reference/acero.html).\n\n## The parquet file format\n\nWhen working with small data sets, csv files can be convenient: they're human readable plain text files, and most analysis tools can read data from a csv.\nHowever, there are some drawbacks:\n\n-   Ambiguity.\n    A csv file does not uniquely specify the data structure.\n    For example, a csv reader has to guess whether `\"08-10-2022\"` should be parsed as a string or a date.\n    If it is a date, it has to guess whether the date refers to October 8th or August 10th.\n    Here is a slightly contrived example.\n    Here is a small tibble that contains some integers and some strings that look suspiciously like dates:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    original <- tibble(\n      integers = 1L:3L,\n      strings = c(\"2022-01-01\", \"1980-12-01\", \"1900-12-12\")\n    )\n    original\n    #> # A tibble: 3 × 2\n    #>   integers strings   \n    #>      <int> <chr>     \n    #> 1        1 2022-01-01\n    #> 2        2 1980-12-01\n    #> 3        3 1900-12-12\n    ```\n    :::\n\n\n    Simple though it is, this data set poses problems for a csv file.\n    The csv version of this data does not explicitly specify that the first column contains integers and the second column contains strings.\n    This produces ambiguity because the csv reader guesses that the data types are double and character respectively:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    csv_file <- tempfile(fileext = \".csv\")\n    write_csv(original, csv_file)\n    read_csv(csv_file, show_col_types = FALSE)\n    #> # A tibble: 3 × 2\n    #>   integers strings   \n    #>      <dbl> <date>    \n    #> 1        1 2022-01-01\n    #> 2        2 1980-12-01\n    #> 3        3 1900-12-12\n    ```\n    :::\n\n\n    The `read_csv()` and `write_csv()` functions are designed to avoid this issue to an extent, but it is impossible to avoid it completely because the csv format is inherently ambiguous.\n\n-   Large size.\n    A csv file is not compressed by default, and tends to be much larger than file formats that use more economical coding and compression tools.\n\n-   Poor performance.\n    Compared to other formats, csv files can be slow to read and write.\n    This is partly due to the larger file taking longer to read from disk, but also in part due to the differences between how data are represented on-disk and in-memory.\n\n-   Inaccuracy.\n    Plain text files store binary numeric data inaccurately due to precision limits.\n    We can verify this by writing 1000 random numbers to a csv file and then reloading the data from the file\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    csv_file <- tempfile(fileext = \".csv\")\n    original <- tibble(old_x = rnorm(1000))\n    write_csv(original, csv_file)\n    reloaded <- read_csv(csv_file) |>\n      rename(new_x = old_x)\n    ```\n    :::\n\n\n    Now let's compare the two, counting the number of times the original value is strictly identical to the reloaded one:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    bind_cols(original, reloaded) |> \n      mutate(same_value = old_x == new_x) |>\n      count(same_value)\n    #> # A tibble: 2 × 2\n    #>   same_value     n\n    #>   <lgl>      <int>\n    #> 1 FALSE        156\n    #> 2 TRUE         844\n    ```\n    :::\n\n\n    A substantial proportion of the reloaded values differ from the original ones.\n    These differences are very small in magnitude, and occur because the csv represents numeric values with limited precision.\n    Even so this can still be a source of problems in your analysis code.\n\nThe Apache Parquet file format ([parquet.apache.org](https://parquet.apache.org/)) is designed to address these limitations.\nLike a csv file, a parquet file stores a single rectangular data set.\nFeatures of the parquet format that make it appealing for data scientists include:\n\n-   Parquet is an open standards-based format widely used by big data systems, so there is little to no risk of vendor lock-in as sometimes occurs with proprietary data formats.\n-   Parquet files store data in a binary format (not plain text) that supports a rich type system, avoiding inaccuracy and ambiguity in data storage.\n-   The Parquet format supports compression, and parquet files commonly use compression to reduce file size. It also relies on [efficient encodings](https://parquet.apache.org/docs/file-format/data-pages/encodings/) to keep file size down.\n-   Parquet files internally break the data into distinct chunks, each with associated metadata. This makes it possible to write intelligent file readers that can jump straight to a specific part of the data when the full data set is not needed.\n\nAll these factors together make the parquet format appealing when working with larger data sets.\n\nThe arrow package supplies `read_parquet()` and `write_parquet()` functions that can read and write data in the parquet format.\nWe'll use these functions to explore the advantages to parquet files.\n\n### Parquet files are unambiguous\n\nRecall this example from earlier:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal <- tibble(\n  integers = 1L:3L,\n  strings = c(\"2022-01-01\", \"1980-12-01\", \"1900-12-12\")\n)\noriginal\n#> # A tibble: 3 × 2\n#>   integers strings   \n#>      <int> <chr>     \n#> 1        1 2022-01-01\n#> 2        2 1980-12-01\n#> 3        3 1900-12-12\n```\n:::\n\n\nAs we saw, writing this data to a csv file and reading it back in leads to a mistake in identifying the data types for each column.\nThe parquet format avoids this ambiguity.\nWhen we write the data set to a parquet file using `write_parquet()` and then read it back in with `read_parquet()`, the data types are preserved:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file <- tempfile(fileext = \".parquet\")\nwrite_parquet(original, parquet_file)\nread_parquet(parquet_file)\n#> # A tibble: 3 × 2\n#>   integers strings   \n#>      <int> <chr>     \n#> 1        1 2022-01-01\n#> 2        2 1980-12-01\n#> 3        3 1900-12-12\n```\n:::\n\n\n### Parquet files are binary\n\nEarlier in this section we saw an example in which csv files store numeric data inaccurately because they use plain text rather than binary format.\nLet's repeat the exercise using the binary parquet format.\nWe first write some random numbers to a parquet file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file <- tempfile(fileext = \".parquet\")\noriginal <- tibble(old_x = rnorm(1000))\nwrite_parquet(original, parquet_file)\n```\n:::\n\n\nNext we load the file and give the reloaded variable a new name:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded <- read_parquet(parquet_file) |>\n  rename(new_x = old_x)\n```\n:::\n\n\nFinally we bind the old and new data sets together and compare them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_cols(original, reloaded) |> \n  mutate(same_value = old_x == new_x) |>\n  count(same_value)\n#> # A tibble: 1 × 2\n#>   same_value     n\n#>   <lgl>      <int>\n#> 1 TRUE        1000\n```\n:::\n\n\nAs you can see, numeric values stored in parquet files are stored faithfully.\nLoading the data from file produces the same values in the original data set.\n\n### Parquet files are smaller\n\nWe can use the Seattle library data set to highlight the advantage in file size.\nThe `write_parquet()` function can take an Arrow Dataset object like `seattle_lib` as the input, so we can write the file like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_parquet(seattle_lib, \"data/seattle-library-checkouts.parquet\")\n```\n:::\n\n\nNow let's compare the two files in size:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.size(\"data/seattle-library-checkouts.csv\")\n#> [1] 9211969044\nfile.size(\"data/seattle-library-checkouts.parquet\")\n#> [1] 4403674061\n```\n:::\n\n\nThe parquet file is about half the size of the csv file.\n\n### Parquet files are faster\n\nThe format of your data file can have a substantial impact on how long a data analysis pipeline takes to execute.\nTo see this, let's time how long it takes to count the number of books checked out in each month of 2021 when the data are stored in csv format:\n\n\n::: {.cell hash='binary-data-formats_cache/html/dataset-performance-csv_5f73293bf68b845adbe3198fc11801d9'}\n\n```{.r .cell-code}\nsystem.time({\n  open_dataset(\n    sources = \"data/seattle-library-checkouts.csv\", \n    format = \"csv\"\n  ) |>\n    filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n    group_by(CheckoutMonth) |>\n    summarise(TotalCheckouts = sum(Checkouts)) |>\n    arrange(desc(CheckoutMonth)) |>\n    collect()\n})\n#>    user  system elapsed \n#>  14.027   2.489  14.158\n```\n:::\n\n\nNow let's repeat the exercise using the parquet file:\n\n\n::: {.cell hash='binary-data-formats_cache/html/dataset-performance-parquet_6cfe5b58341917dbbf44d0b1becfab4a'}\n\n```{.r .cell-code}\nsystem.time({\n  open_dataset(\n    sources = \"data/seattle-library-checkouts.parquet\", \n    format = \"parquet\"\n  ) |>\n    filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n    group_by(CheckoutMonth) |>\n    summarise(TotalCheckouts = sum(Checkouts)) |>\n    arrange(desc(CheckoutMonth)) |>\n    collect()\n})\n#>    user  system elapsed \n#>   2.129   0.663   1.368\n```\n:::\n\n\nThe analysis of the parquet data finishes in about one tenth the time it takes to perform the same analysis with the csv data.\n\n## Multi-file data sets\n\nWhen data sets become large it is often useful to split the data across many files rather than storing everything in a single large file.\nWhen this structuring is done intelligently, the multi-file strategy can lead to significant improvements in performance.\nWe'll illustrate this using the Seattle library data and the arrow package.\n\n### Writing multi-file data\n\nWhen deciding to pursue a multi-file strategy, the first task is to decide how to partition your data set.\nIf you break the data set into too few files you won't gain the performance improvements you're hoping for, but if you break it into too many you won't gain much either.\nIf you split the data in a meaningful way, you'll get some performance advantages, but if you split the data randomly you probably won't see a lot of gains.\nThere are no hard and fast rules about how to partition a data set: the results will depend on your data, access patterns, and the systems that read the data.\nAs a rough guide, the arrow package documentation suggests the following heuristics:\n\n-   Avoid files smaller than 20MB and larger than 2GB.\n-   Avoid partitioning layouts that produce more than 10,000 distinct files.\n\nAccording to these guidelines, the Seattle library data is a little too large to be stored as a single 4GB parquet file.\nWe should consider splitting it up.\n\nAs a general rule, a useful strategy for partitioning a large dataset is to create meaningful subsets based on one or more variable that you're likely to use to filter the data.\nFor the library data it seems likely that we would often want to analyze the data separately by year, so the `CheckoutYear` column seems like a good candidate.\n\nPartitioning a data set can be done using the `write_dataset()` function from the arrow package, which respects the grouping created by the dplyr `group_by()` function.\nTo create a multi-file data set with one parquet file per year of library data we would do the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_lib |>\n  group_by(CheckoutYear) |>\n  write_dataset(\n    path = \"data/seattle-library-checkouts\",\n    format = \"parquet\"\n  )\n```\n:::\n\n\nLet's take a look at the files:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_lib_files <- list.files(\n  path = \"data/seattle-library-checkouts\",\n  recursive = TRUE\n) \nseattle_lib_files\n#>  [1] \"CheckoutYear=2005/part-0.parquet\" \"CheckoutYear=2006/part-0.parquet\"\n#>  [3] \"CheckoutYear=2007/part-0.parquet\" \"CheckoutYear=2008/part-0.parquet\"\n#>  [5] \"CheckoutYear=2009/part-0.parquet\" \"CheckoutYear=2010/part-0.parquet\"\n#>  [7] \"CheckoutYear=2011/part-0.parquet\" \"CheckoutYear=2012/part-0.parquet\"\n#>  [9] \"CheckoutYear=2013/part-0.parquet\" \"CheckoutYear=2014/part-0.parquet\"\n#> [11] \"CheckoutYear=2015/part-0.parquet\" \"CheckoutYear=2016/part-0.parquet\"\n#> [13] \"CheckoutYear=2017/part-0.parquet\" \"CheckoutYear=2018/part-0.parquet\"\n#> [15] \"CheckoutYear=2019/part-0.parquet\" \"CheckoutYear=2020/part-0.parquet\"\n#> [17] \"CheckoutYear=2021/part-0.parquet\" \"CheckoutYear=2022/part-0.parquet\"\n```\n:::\n\n\nThese filenames follow a \"self-describing\" convention used by the Apache Hive project ([hive.apache.org](https://hive.apache.org/)).\nHive-style partitions name folders using a \"key=value\" convention, and the data files in that folder contain the subset of the data for which the key has the relevant value.\nFollowing this convention makes it easier to locate files relevant to a particular query.\n\n### Performance advantages\n\nPartitioning a large data set into many smaller, meaningful files can produce substantial improvements in performance.\nTo illustrate, we'll repeat the analysis of the 2021 Seattle library book checkouts by month using the partitioned data:\n\n\n::: {.cell hash='binary-data-formats_cache/html/dataset-performance-multiple-parquet_99625178da5d3354e2f84f60118a70ec'}\n\n```{.r .cell-code}\nsystem.time({\n  open_dataset(\n    sources = \"data/seattle-library-checkouts/\", \n    format = \"parquet\"\n  ) |>\n    filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n    group_by(CheckoutMonth) |>\n    summarise(TotalCheckouts = sum(Checkouts)) |>\n    arrange(desc(CheckoutMonth)) |>\n    collect()\n})\n#>    user  system elapsed \n#>   0.430   0.063   0.157\n```\n:::\n\n\nThis is a 10-fold speed up compared to a single parquet file, and a 100-fold speed up compared to the original csv file.\n\n## Other topics\n\nI haven't quite worked out the best way to integrate these sections.\n\n### Using duckdb with arrow\n\nOne useful feature of arrow is that it is interoperable with duckdb and dbplyr.\nIt is possible to open a multi-file data set using arrow and then pass it to duckdb using the `to_duckdb()` function.\nDoing so means that your dplyr code will now be interpreted by dbplyr (rather than arrow) and executed by duckdb.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(\"data/seattle-library-checkouts/\") |>\n  to_duckdb() |>\n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarise(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect()\n#> Warning: Missing values are always removed in SQL aggregation functions.\n#> Use `na.rm = TRUE` to silence this warning\n#> This warning is displayed once every 8 hours.\n#> # A tibble: 12 × 2\n#>   CheckoutMonth TotalCheckouts\n#>           <dbl>          <dbl>\n#> 1            12         192506\n#> 2            11         245485\n#> 3            10         244967\n#> 4             9         241336\n#> 5             8         229374\n#> 6             7         224132\n#> # … with 6 more rows\n```\n:::\n\n\nThere is an analogous function `to_arrow()` that passes control back to arrow, meaning that it is possible to swap back and forth between the duckdb and arrow back ends within a single analysis pipeline.\nThis operation is extremely efficient because both libraries deal with the Arrow format, so data is passed between then without making a copy.\nThis can occasionally be valuable when one tool implements functionality that the other does not.\n\n### Data in the cloud\n\nSometimes when working with large datasets you encounter the situation when data files are stored remotely on cloud storage services such as Amazon's Simple Storage Service (S3) or Google Cloud Storage (GCS).\nFunctions in arrow that read and write data, including `open_dataset()`, are able to read and write from remote data sources with the help of the `s3_bucket()` function (for Amazon S3) or the `gs_bucket()` function (for Google cloud).\nAn example is shown below:\n\n\n::: {.cell hash='binary-data-formats_cache/html/open-s3-dataset_d255340c5735c636dcf01084f4f0f55c'}\n\n```{.r .cell-code}\ndiamonds <- s3_bucket(\"voltrondata-labs-datasets/diamonds\")\ndiamonds |> \n  open_dataset() |>\n  glimpse()\n#> FileSystemDataset with 5 Parquet files\n#> 53,940 rows x 10 columns\n#> $ carat            <double> 0.22, 0.86, 0.96, 0.70, 0.70, 0.91, 0.91, 0.98, 0.84…\n#> $ color   <dictionary<...>> E, E, F, F, F, H, H, H, G, E, J, G, E, G, I, E, F, G…\n#> $ clarity <dictionary<...>> VS2, SI2, SI2, VS2, VS2, SI2, SI2, SI2, SI1, I1, SI2…\n#> $ depth            <double> 65.1, 55.1, 66.3, 64.5, 65.3, 64.4, 65.7, 67.9, 55.1…\n#> $ table            <double> 61, 69, 62, 57, 55, 57, 60, 60, 67, 58, 59, 66, 60, …\n#> $ price             <int32> 337, 2757, 2759, 2762, 2762, 2763, 2763, 2777, 2782,…\n#> $ x                <double> 3.87, 6.45, 6.27, 5.57, 5.63, 6.11, 6.03, 6.05, 6.39…\n#> $ y                <double> 3.78, 6.33, 5.95, 5.53, 5.58, 6.09, 5.99, 5.97, 6.20…\n#> $ z                <double> 2.49, 3.52, 4.07, 3.58, 3.66, 3.93, 3.95, 4.08, 3.47…\n#> $ cut              <string> \"Fair\", \"Fair\", \"Fair\", \"Fair\", \"Fair\", \"Fair\", \"Fai…\n#> Call `print()` for full schema details\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}