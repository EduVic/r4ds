# Binary data formats {#sec-binary-data}

```{r setup}
#| results: "asis"
#| echo: false
source("_common.R")
status("drafting")
```

## Introduction to arrow

-   out of memory, fast
-   cross-language

## The Seattle library data

The data set is available at [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6), formatted as a single 9GB csv file.
Let's suppose we have downloaded this file to `data/seattle-library-checkout.csv`.
The data set is large enough that we might not want to try loading the entire thing into memory.
Instead, we'll use `arrow::open_dataset()` to scan the file on-disk and create an Arrow Dataset object called `seattle_lib`:

```{r load-packages-1}
library(tibble)
library(dplyr, warn.conflicts = FALSE)
library(arrow, warn.conflicts = FALSE)
```

```{r open-dataset}
seattle_lib <- open_dataset(
  sources = "data/seattle-library-checkouts.csv", 
  format = "csv"
)
```

This object doesn't store the library checkout data in memory, which we can verify by comparing the on-disk size calculated by `fs::file_size()` to the in-memory size calculated by `lobstr::obj_size()`:

```{r compare-sizes}
#| cache: true
fs::file_size("data/seattle-library-checkouts.csv")
lobstr::obj_size(seattle_lib)
```

The `seattle_lib` Dataset occupies 13MB in-memory, much smaller than the on-disk file size of 9GB.

Though the data remain on disk and are not stored in R, it is still possible to interact with a Dataset using dplyr functions as if it were.
Let's use `glimpse()` to take a look at the data:

```{r glimpse-data}
#| cache: true
glimpse(seattle_lib)
```

## Reencoding the data as a parquet file

```{r write-parquet}
#| eval: false
write_parquet(seattle_lib, "data/seattle-library-checkouts.parquet")
```

The file is smaller:

```{r}
fs::file_size("data/seattle-library-checkouts.parquet")
```

## Splitting the data across multiple parquet files

```{r write-dataset}
#| eval: false
seattle_lib |>
  group_by(CheckoutYear) |>
  write_dataset(
    path = "data/seattle-library-checkouts",
    format = "parquet"
  )
```

Let's take a look at the files:

```{r show-parquet-files}
seattle_lib_files <- fs::dir_ls(
  path = "data/seattle-library-checkouts",
  type = "file",
  recurse = TRUE
) 
seattle_lib_files
```

-   show how to convert to parquet
-   something general about the multiple files strategy
-   mention arrow (e.g.) S3 support

## Using dplyr with arrow

Find the top 10 books in 2021:

```{r arrow-dplyr-example}
open_dataset("data/seattle-library-checkouts/") |>
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |> 
  group_by(Title) |> 
  summarize(AnnualCheckouts = sum(Checkouts)) |> 
  ungroup() |>
  slice_max(AnnualCheckouts, n = 10, with_ties = FALSE) |>
  collect()
```

-   it's just dplyr
-   collect()
-   things that don't work (yet)

## Performance advantages

```{r top-10-function}
top_10_books <- function(source, format) {
  open_dataset(source, format = format) |>
    filter(CheckoutYear == 2021, MaterialType == "BOOK") |> 
    group_by(Title) |> 
    summarize(AnnualCheckouts = sum(Checkouts)) |> 
    ungroup() |>
    slice_max(AnnualCheckouts, n = 10, with_ties = FALSE) |>
    collect()
}
```

```{r dataset-performance-csv}
#| cache: true
system.time(
  top_10_books("data/seattle-library-checkouts.csv", "csv")
)
```

```{r dataset-performance-parquet}
#| cache: true
system.time(
  top_10_books("data/seattle-library-checkouts.parquet", "parquet")
)
```

```{r dataset-performance-multiple-parquet}
#| cache: true
system.time(
  top_10_books("data/seattle-library-checkouts", "parquet")
)
```

## Using duckdb with arrow

```{r load-packages-2}
library(duckdb)
library(dbplyr)
```

```{r use-duckdb}
open_dataset("data/seattle-library-checkouts/") |>
  to_duckdb() |>
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |> 
  group_by(Title) |> 
  summarize(AnnualCheckouts = sum(Checkouts)) |> 
  ungroup() |>
  slice_max(AnnualCheckouts, n = 10, with_ties = FALSE) |>
  collect()
```

-   show how you can also access via duckdb and dbplyr
-   keep it brief; point is to illustrate using same data source in multiple tools
