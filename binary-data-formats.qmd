# Binary data formats {#sec-binary-data}

```{r}
#| results: "asis"
#| echo: false
source("_common.R")
status("drafting")
```

## Introduction to arrow

-   out of memory, fast
-   cross-language

## The Seattle library data

The data set is available at [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6), formatted as a single 9GB csv file.
Let's suppose we have downloaded this file to `data/seattle-library-checkout.csv`.
The data set is large enough that we might not want to try loading the entire thing into memory.
Instead, we'll use `arrow::open_dataset()` to scan the file on-disk and create an Arrow Dataset object called `seattle_lib`:

```{r}
library(tibble)
library(dplyr, warn.conflicts = FALSE)
library(arrow, warn.conflicts = FALSE)

seattle_lib <- open_dataset(
  sources = "data/seattle-library-checkouts.csv", 
  format = "csv"
)
```

This object doesn't store the library checkout data in memory, which we can verify by comparing the on-disk size calculated by `fs::file_size()` to the in-memory size calculated by `lobstr::obj_size()`:

```{r}
fs::file_size("data/seattle-library-checkouts.csv")
lobstr::obj_size(seattle_lib)
```

The `seattle_lib` Dataset occupies 13MB in-memory, much smaller than the on-disk file size of 9GB.

Though the data remain on disk and are not stored in R, it is still possible to interact with a Dataset using dplyr functions as if it were.
Let's use `glimpse()` to take a look at the data:

```{r}
glimpse(seattle_lib)
```

-   how to get the csv files (the pain of big data)
-   show how to convert to parquet
-   something general about the multiple files strategy
-   mention arrow (e.g.) S3 support

## Using dplyr with arrow

-   it's just dplyr
-   collect()
-   things that don't work (yet)

## Using duckdb with arrow

-   show how you can also access via duckdb and dbplyr
-   keep it brief; point is to illustrate using same data source in multiple tools
